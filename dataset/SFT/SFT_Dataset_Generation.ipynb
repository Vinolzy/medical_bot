{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://community.openai.com/t/error-with-openai-1-56-0-client-init-got-an-unexpected-keyword-argument-proxies/1040332/11\n",
    "!pip install openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e2d94f9bc7780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145c3b0f91336826",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca41f9eff4753cc",
   "metadata": {},
   "source": [
    "login huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1251e86ba236a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "\n",
    "HF_Token = \"your own key\"\n",
    "login(token=HF_Token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b92beb8fd5c00c",
   "metadata": {},
   "source": [
    "dataset: https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9c5c0c8cfbf44e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T08:10:16.736338Z",
     "start_time": "2025-08-07T08:10:15.894031Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\soft\\python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "def convert_dataset_to_sft_format():\n",
    "    \"\"\"\n",
    "    Download and convert HuggingFace dataset to SFT format\n",
    "    \"\"\"\n",
    "    print(\"Downloading dataset...\")\n",
    "\n",
    "    # Load dataset\n",
    "    try:\n",
    "        ds = load_dataset(\"lavita/ChatDoctor-HealthCareMagic-100k\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Dataset loaded successfully, total samples: {len(ds['train'])}\")\n",
    "\n",
    "    # System prompt\n",
    "    system_content = (\"You are a highly professional medical assistant. Your task is to provide accurate and well-structured medical information based on a patient's symptoms. Your tone must be objective and professional. Always start with a polite greeting and end with a supportive, yet professional, closing. When discussing possible diagnoses or treatments, it is critical to state that this is for informational purposes only and strongly recommend a consultation with a healthcare professional.\")\n",
    "\n",
    "    # Convert data format\n",
    "    converted_data = []\n",
    "\n",
    "    for i, sample in enumerate(ds['train']):\n",
    "        # Check if required fields exist\n",
    "        if 'input' not in sample or 'output' not in sample:\n",
    "            print(f\"Warning: Sample {i} missing required fields, skipping\")\n",
    "            continue\n",
    "\n",
    "        # Build message format\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_content\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sample['input']\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": sample['output']\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Add to result list\n",
    "        converted_data.append({\"messages\": messages})\n",
    "\n",
    "        # Show progress every 1000 samples\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"Processed {i + 1} samples...\")\n",
    "\n",
    "    # Save as JSON file\n",
    "    output_file = \"sft_dataset.json\"\n",
    "\n",
    "    print(f\"Saving to {output_file}...\")\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for item in converted_data:\n",
    "                # One JSON object per line\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "        print(f\"Conversion completed!\")\n",
    "        print(f\"- Total processed samples: {len(converted_data)}\")\n",
    "        print(f\"- Output file: {output_file}\")\n",
    "        print(f\"- Format: One JSON object per line\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c5ff18fc73b340",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T08:10:22.602550Z",
     "start_time": "2025-08-07T08:10:22.597537Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate_output_format(file_path=\"sft_dataset.json\", num_samples=3):\n",
    "    \"\"\"\n",
    "    Validate output file format\n",
    "    \"\"\"\n",
    "    print(f\"\\nValidating output file format...\")\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= num_samples:\n",
    "                    break\n",
    "\n",
    "                data = json.loads(line.strip())\n",
    "                print(f\"\\nSample {i + 1}:\")\n",
    "                print(json.dumps(data, ensure_ascii=False, indent=2))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e192c99d63515f1",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3724a6f8996641e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T08:10:58.308888Z",
     "start_time": "2025-08-07T08:10:26.239093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\soft\\python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\23104\\.cache\\huggingface\\hub\\datasets--lavita--ChatDoctor-HealthCareMagic-100k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112165/112165 [00:00<00:00, 1113421.79 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully, total samples: 112165\n",
      "Processed 1000 samples...\n",
      "Processed 2000 samples...\n",
      "Processed 3000 samples...\n",
      "Processed 4000 samples...\n",
      "Processed 5000 samples...\n",
      "Processed 6000 samples...\n",
      "Processed 7000 samples...\n",
      "Processed 8000 samples...\n",
      "Processed 9000 samples...\n",
      "Processed 10000 samples...\n",
      "Processed 11000 samples...\n",
      "Processed 12000 samples...\n",
      "Processed 13000 samples...\n",
      "Processed 14000 samples...\n",
      "Processed 15000 samples...\n",
      "Processed 16000 samples...\n",
      "Processed 17000 samples...\n",
      "Processed 18000 samples...\n",
      "Processed 19000 samples...\n",
      "Processed 20000 samples...\n",
      "Processed 21000 samples...\n",
      "Processed 22000 samples...\n",
      "Processed 23000 samples...\n",
      "Processed 24000 samples...\n",
      "Processed 25000 samples...\n",
      "Processed 26000 samples...\n",
      "Processed 27000 samples...\n",
      "Processed 28000 samples...\n",
      "Processed 29000 samples...\n",
      "Processed 30000 samples...\n",
      "Processed 31000 samples...\n",
      "Processed 32000 samples...\n",
      "Processed 33000 samples...\n",
      "Processed 34000 samples...\n",
      "Processed 35000 samples...\n",
      "Processed 36000 samples...\n",
      "Processed 37000 samples...\n",
      "Processed 38000 samples...\n",
      "Processed 39000 samples...\n",
      "Processed 40000 samples...\n",
      "Processed 41000 samples...\n",
      "Processed 42000 samples...\n",
      "Processed 43000 samples...\n",
      "Processed 44000 samples...\n",
      "Processed 45000 samples...\n",
      "Processed 46000 samples...\n",
      "Processed 47000 samples...\n",
      "Processed 48000 samples...\n",
      "Processed 49000 samples...\n",
      "Processed 50000 samples...\n",
      "Processed 51000 samples...\n",
      "Processed 52000 samples...\n",
      "Processed 53000 samples...\n",
      "Processed 54000 samples...\n",
      "Processed 55000 samples...\n",
      "Processed 56000 samples...\n",
      "Processed 57000 samples...\n",
      "Processed 58000 samples...\n",
      "Processed 59000 samples...\n",
      "Processed 60000 samples...\n",
      "Processed 61000 samples...\n",
      "Processed 62000 samples...\n",
      "Processed 63000 samples...\n",
      "Processed 64000 samples...\n",
      "Processed 65000 samples...\n",
      "Processed 66000 samples...\n",
      "Processed 67000 samples...\n",
      "Processed 68000 samples...\n",
      "Processed 69000 samples...\n",
      "Processed 70000 samples...\n",
      "Processed 71000 samples...\n",
      "Processed 72000 samples...\n",
      "Processed 73000 samples...\n",
      "Processed 74000 samples...\n",
      "Processed 75000 samples...\n",
      "Processed 76000 samples...\n",
      "Processed 77000 samples...\n",
      "Processed 78000 samples...\n",
      "Processed 79000 samples...\n",
      "Processed 80000 samples...\n",
      "Processed 81000 samples...\n",
      "Processed 82000 samples...\n",
      "Processed 83000 samples...\n",
      "Processed 84000 samples...\n",
      "Processed 85000 samples...\n",
      "Processed 86000 samples...\n",
      "Processed 87000 samples...\n",
      "Processed 88000 samples...\n",
      "Processed 89000 samples...\n",
      "Processed 90000 samples...\n",
      "Processed 91000 samples...\n",
      "Processed 92000 samples...\n",
      "Processed 93000 samples...\n",
      "Processed 94000 samples...\n",
      "Processed 95000 samples...\n",
      "Processed 96000 samples...\n",
      "Processed 97000 samples...\n",
      "Processed 98000 samples...\n",
      "Processed 99000 samples...\n",
      "Processed 100000 samples...\n",
      "Processed 101000 samples...\n",
      "Processed 102000 samples...\n",
      "Processed 103000 samples...\n",
      "Processed 104000 samples...\n",
      "Processed 105000 samples...\n",
      "Processed 106000 samples...\n",
      "Processed 107000 samples...\n",
      "Processed 108000 samples...\n",
      "Processed 109000 samples...\n",
      "Processed 110000 samples...\n",
      "Processed 111000 samples...\n",
      "Processed 112000 samples...\n",
      "Saving to sft_dataset.json...\n",
      "Conversion completed!\n",
      "- Total processed samples: 112165\n",
      "- Output file: sft_dataset.json\n",
      "- Format: One JSON object per line\n",
      "\n",
      "Validating output file format...\n",
      "\n",
      "Sample 1:\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You are a highly professional medical assistant. Your task is to provide accurate and well-structured medical information based on a patient's symptoms. Your tone must be objective and professional. Always start with a polite greeting and end with a supportive, yet professional, closing. When discussing possible diagnoses or treatments, it is critical to state that this is for informational purposes only and strongly recommend a consultation with a healthcare professional.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, the spinning lessen so i am not sure whether its connected or coincidences.. Thank you doc!\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Hi, Thank you for posting your query. The most likely cause for your symptoms is benign paroxysmal positional vertigo (BPPV), a type of peripheral vertigo. In this condition, the most common symptom is dizziness or giddiness, which is made worse with movements. Accompanying nausea and vomiting are common. The condition is due to problem in the ear, and improves in a few days on own. Betahistine tablets would help relieve your symptoms. Doing vestibular rehabilitation or adaptation exercises would prevent the recurrence of these symptoms. An ENT evaluation would also help. I hope it helps. Best wishes, Chat Doctor.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Sample 2:\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You are a highly professional medical assistant. Your task is to provide accurate and well-structured medical information based on a patient's symptoms. Your tone must be objective and professional. Always start with a polite greeting and end with a supportive, yet professional, closing. When discussing possible diagnoses or treatments, it is critical to state that this is for informational purposes only and strongly recommend a consultation with a healthcare professional.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"My baby has been pooing 5-6 times a day for a week. In the last few days it has increased to 7 and they are very watery with green stringy bits in them. He does not seem unwell i.e no temperature and still eating. He now has a very bad nappy rash from the pooing ...help!\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Hi... Thank you for consulting in Chat Doctor. It seems your kid is having viral diarrhea. Once it starts it will take 5-7 days to completely get better. Unless the kids having low urine output or very dull or excessively sleepy or blood in motion or green bilious vomiting...you need not worry. There is no need to use antibiotics unless there is blood in the motion. Antibiotics might worsen if unnecessarily used causing antibiotic associated diarrhea. I suggest you use zinc supplements (Z&D Chat Doctor.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Sample 3:\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You are a highly professional medical assistant. Your task is to provide accurate and well-structured medical information based on a patient's symptoms. Your tone must be objective and professional. Always start with a polite greeting and end with a supportive, yet professional, closing. When discussing possible diagnoses or treatments, it is critical to state that this is for informational purposes only and strongly recommend a consultation with a healthcare professional.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Hello, My husband is taking Oxycodone due to a broken leg/surgery. He has been taking this pain medication for one month. We are trying to conceive our second baby. Will this medication afect the fetus? Or the health of the baby? Or can it bring birth defects? Thank you.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Hello, and I hope I can help you today.First, there is no medication that can be taken by the father that has any way to get into your system or a baby if you conceive.  Medications can only affect a fetus if you take it while pregnant. The only issue is that certain medications may decrease a men sperm count and affect fertility, however pain medications like Oxycodone do not have this effect. So there is no reason for you to worry about conceiving while taking this medication.  The best way you can prepare for a healthy pregnancy is to follow a well-balanced diet, limit alcohol consumption and avoid cigarette smoke, and take a daily prenatal vitamin or folic acid, as folic acid supplements in early pregnancy helps to prevent certain types of birth defects. I hope this answers your question and best wishes for your upcoming pregnancy,\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "convert_dataset_to_sft_format()\n",
    "validate_output_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10b4500bbf9165b",
   "metadata": {},
   "source": [
    "# max_sequence_length determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e19437466a3735f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T08:33:48.782609Z",
     "start_time": "2025-08-07T08:33:42.595641Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\soft\\python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for meta-llama/Llama-3.2-3B...\n",
      "Failed to load tokenizer: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-3B.\n",
      "401 Client Error. (Request ID: Root=1-689464ee-13e75a4c3ccc2a0a356a0940;bf4be3d5-99ec-433b-a4d0-1c277af84083)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-3B is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "Using approximate character-based estimation (1 token â‰ˆ 4 characters)\n",
      "Analyzing dataset: sft_dataset.json\n",
      "Processed 1000 samples...\n",
      "Processed 2000 samples...\n",
      "Processed 3000 samples...\n",
      "Processed 4000 samples...\n",
      "Processed 5000 samples...\n",
      "Processed 6000 samples...\n",
      "Processed 7000 samples...\n",
      "Processed 8000 samples...\n",
      "Processed 9000 samples...\n",
      "Processed 10000 samples...\n",
      "Processed 11000 samples...\n",
      "Processed 12000 samples...\n",
      "Processed 13000 samples...\n",
      "Processed 14000 samples...\n",
      "Processed 15000 samples...\n",
      "Processed 16000 samples...\n",
      "Processed 17000 samples...\n",
      "Processed 18000 samples...\n",
      "Processed 19000 samples...\n",
      "Processed 20000 samples...\n",
      "Processed 21000 samples...\n",
      "Processed 22000 samples...\n",
      "Processed 23000 samples...\n",
      "Processed 24000 samples...\n",
      "Processed 25000 samples...\n",
      "Processed 26000 samples...\n",
      "Processed 27000 samples...\n",
      "Processed 28000 samples...\n",
      "Processed 29000 samples...\n",
      "Processed 30000 samples...\n",
      "Processed 31000 samples...\n",
      "Processed 32000 samples...\n",
      "Processed 33000 samples...\n",
      "Processed 34000 samples...\n",
      "Processed 35000 samples...\n",
      "Processed 36000 samples...\n",
      "Processed 37000 samples...\n",
      "Processed 38000 samples...\n",
      "Processed 39000 samples...\n",
      "Processed 40000 samples...\n",
      "Processed 41000 samples...\n",
      "Processed 42000 samples...\n",
      "Processed 43000 samples...\n",
      "Processed 44000 samples...\n",
      "Processed 45000 samples...\n",
      "Processed 46000 samples...\n",
      "Processed 47000 samples...\n",
      "Processed 48000 samples...\n",
      "Processed 49000 samples...\n",
      "Processed 50000 samples...\n",
      "Processed 51000 samples...\n",
      "Processed 52000 samples...\n",
      "Processed 53000 samples...\n",
      "Processed 54000 samples...\n",
      "Processed 55000 samples...\n",
      "Processed 56000 samples...\n",
      "Processed 57000 samples...\n",
      "Processed 58000 samples...\n",
      "Processed 59000 samples...\n",
      "Processed 60000 samples...\n",
      "Processed 61000 samples...\n",
      "Processed 62000 samples...\n",
      "Processed 63000 samples...\n",
      "Processed 64000 samples...\n",
      "Processed 65000 samples...\n",
      "Processed 66000 samples...\n",
      "Processed 67000 samples...\n",
      "Processed 68000 samples...\n",
      "Processed 69000 samples...\n",
      "Processed 70000 samples...\n",
      "Processed 71000 samples...\n",
      "Processed 72000 samples...\n",
      "Processed 73000 samples...\n",
      "Processed 74000 samples...\n",
      "Processed 75000 samples...\n",
      "Processed 76000 samples...\n",
      "Processed 77000 samples...\n",
      "Processed 78000 samples...\n",
      "Processed 79000 samples...\n",
      "Processed 80000 samples...\n",
      "Processed 81000 samples...\n",
      "Processed 82000 samples...\n",
      "Processed 83000 samples...\n",
      "Processed 84000 samples...\n",
      "Processed 85000 samples...\n",
      "Processed 86000 samples...\n",
      "Processed 87000 samples...\n",
      "Processed 88000 samples...\n",
      "Processed 89000 samples...\n",
      "Processed 90000 samples...\n",
      "Processed 91000 samples...\n",
      "Processed 92000 samples...\n",
      "Processed 93000 samples...\n",
      "Processed 94000 samples...\n",
      "Processed 95000 samples...\n",
      "Processed 96000 samples...\n",
      "Processed 97000 samples...\n",
      "Processed 98000 samples...\n",
      "Processed 99000 samples...\n",
      "Processed 100000 samples...\n",
      "Processed 101000 samples...\n",
      "Processed 102000 samples...\n",
      "Processed 103000 samples...\n",
      "Processed 104000 samples...\n",
      "Processed 105000 samples...\n",
      "Processed 106000 samples...\n",
      "Processed 107000 samples...\n",
      "Processed 108000 samples...\n",
      "Processed 109000 samples...\n",
      "Processed 110000 samples...\n",
      "Processed 111000 samples...\n",
      "Processed 112000 samples...\n",
      "\n",
      "=== Sequence Length Analysis Results ===\n",
      "Total samples analyzed: 112165\n",
      "Tokenizer used: Character-based estimation\n",
      "\n",
      "ðŸ“Š INPUT LENGTH STATS (System + User + Special Tokens):\n",
      "  Min: 125 tokens\n",
      "  Max: 2922 tokens\n",
      "  Mean: 229.8 tokens\n",
      "  Median: 212.0 tokens\n",
      "  90th percentile: 291 tokens\n",
      "  95th percentile: 333 tokens\n",
      "  99th percentile: 463 tokens\n",
      "\n",
      "ðŸ“Š TOTAL LENGTH STATS (Input + Assistant):\n",
      "  Min: 131 tokens\n",
      "  Max: 3063 tokens\n",
      "  Mean: 380.1 tokens\n",
      "  Median: 364.0 tokens\n",
      "  90th percentile: 482 tokens\n",
      "  95th percentile: 536 tokens\n",
      "  99th percentile: 682 tokens\n",
      "\n",
      "ðŸŽ¯ COVERAGE ANALYSIS (% samples that fit within max_seq_length):\n",
      "Based on INPUT length (System + User):\n",
      "  max_seq_length=512: 99.4% of samples\n",
      "  max_seq_length=1024: 100.0% of samples\n",
      "  max_seq_length=1536: 100.0% of samples\n",
      "  max_seq_length=2048: 100.0% of samples\n",
      "  max_seq_length=3072: 100.0% of samples\n",
      "  max_seq_length=4096: 100.0% of samples\n",
      "\n",
      "Based on TOTAL length (Input + Assistant):\n",
      "  max_seq_length=512: 93.3% of samples\n",
      "  max_seq_length=1024: 99.9% of samples\n",
      "  max_seq_length=1536: 100.0% of samples\n",
      "  max_seq_length=2048: 100.0% of samples\n",
      "  max_seq_length=3072: 100.0% of samples\n",
      "  max_seq_length=4096: 100.0% of samples\n",
      "\n",
      "ðŸ’¡ RECOMMENDATIONS:\n",
      "  For 95% input coverage: max_seq_length = 384\n",
      "  For 95% total coverage: max_seq_length = 640\n",
      "  Current setting (1536): covers 100.0% of inputs\n",
      "  âœ… Your current max_seq_length=1536 looks good for input coverage\n",
      "\n",
      "ðŸŽ¯ Quick recommendation for your current setup:\n",
      "Your max_seq_length=1536 setting covers approximately:\n",
      "- More than 95% of input sequences\n",
      "- Consider the analysis above to decide if you need to adjust it\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "SFT Max Sequence Length Analyzer\n",
    "Analyze dataset to determine optimal max_seq_length for SFT training\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_sequence_lengths(dataset_file=\"sft_dataset.json\", model_name=\"meta-llama/Llama-3.2-3B\"):\n",
    "    \"\"\"\n",
    "    Analyze sequence lengths in the dataset to determine optimal max_seq_length\n",
    "\n",
    "    Args:\n",
    "        dataset_file: Path to the SFT dataset JSON file\n",
    "        model_name: Model name for tokenizer (should match your target model)\n",
    "\n",
    "    Returns:\n",
    "        dict: Analysis results with recommendations\n",
    "    \"\"\"\n",
    "    print(f\"Loading tokenizer for {model_name}...\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load tokenizer: {e}\")\n",
    "        print(\"Using approximate character-based estimation (1 token â‰ˆ 4 characters)\")\n",
    "        tokenizer = None\n",
    "\n",
    "    print(f\"Analyzing dataset: {dataset_file}\")\n",
    "\n",
    "    # Statistics containers\n",
    "    input_lengths = []  # system + user message lengths\n",
    "    total_lengths = []  # system + user + assistant message lengths\n",
    "    assistant_lengths = []  # assistant message lengths only\n",
    "\n",
    "    sample_count = 0\n",
    "\n",
    "    try:\n",
    "        with open(dataset_file, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                try:\n",
    "                    data = json.loads(line.strip())\n",
    "                    messages = data.get('messages', [])\n",
    "\n",
    "                    if len(messages) < 3:\n",
    "                        print(f\"Warning: Line {line_num} has fewer than 3 messages, skipping\")\n",
    "                        continue\n",
    "\n",
    "                    # Extract messages\n",
    "                    system_msg = messages[0]['content'] if messages[0]['role'] == 'system' else \"\"\n",
    "                    user_msg = messages[1]['content'] if messages[1]['role'] == 'user' else \"\"\n",
    "                    assistant_msg = messages[2]['content'] if messages[2]['role'] == 'assistant' else \"\"\n",
    "\n",
    "                    # Calculate token lengths\n",
    "                    if tokenizer:\n",
    "                        # Use actual tokenizer\n",
    "                        system_tokens = len(tokenizer.encode(system_msg, add_special_tokens=False))\n",
    "                        user_tokens = len(tokenizer.encode(user_msg, add_special_tokens=False))\n",
    "                        assistant_tokens = len(tokenizer.encode(assistant_msg, add_special_tokens=False))\n",
    "\n",
    "                        # Add special tokens (approximate)\n",
    "                        special_tokens_overhead = 6  # <|begin_of_text|>, <|start_header_id|>, etc.\n",
    "                    else:\n",
    "                        # Approximate estimation\n",
    "                        system_tokens = len(system_msg) // 4\n",
    "                        user_tokens = len(user_msg) // 4\n",
    "                        assistant_tokens = len(assistant_msg) // 4\n",
    "                        special_tokens_overhead = 6\n",
    "\n",
    "                    # Calculate lengths\n",
    "                    input_length = system_tokens + user_tokens + special_tokens_overhead\n",
    "                    total_length = input_length + assistant_tokens\n",
    "\n",
    "                    input_lengths.append(input_length)\n",
    "                    total_lengths.append(total_length)\n",
    "                    assistant_lengths.append(assistant_tokens)\n",
    "\n",
    "                    sample_count += 1\n",
    "\n",
    "                    if sample_count % 1000 == 0:\n",
    "                        print(f\"Processed {sample_count} samples...\")\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: Invalid JSON on line {line_num}, skipping\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error processing line {line_num}: {e}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Dataset file {dataset_file} not found\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not input_lengths:\n",
    "        print(\"No valid samples found in dataset\")\n",
    "        return None\n",
    "\n",
    "    # Calculate statistics\n",
    "    input_lengths = np.array(input_lengths)\n",
    "    total_lengths = np.array(total_lengths)\n",
    "    assistant_lengths = np.array(assistant_lengths)\n",
    "\n",
    "    results = {\n",
    "        'total_samples': len(input_lengths),\n",
    "        'input_stats': {\n",
    "            'min': int(np.min(input_lengths)),\n",
    "            'max': int(np.max(input_lengths)),\n",
    "            'mean': float(np.mean(input_lengths)),\n",
    "            'median': float(np.median(input_lengths)),\n",
    "            'std': float(np.std(input_lengths)),\n",
    "            'p90': float(np.percentile(input_lengths, 90)),\n",
    "            'p95': float(np.percentile(input_lengths, 95)),\n",
    "            'p99': float(np.percentile(input_lengths, 99))\n",
    "        },\n",
    "        'total_stats': {\n",
    "            'min': int(np.min(total_lengths)),\n",
    "            'max': int(np.max(total_lengths)),\n",
    "            'mean': float(np.mean(total_lengths)),\n",
    "            'median': float(np.median(total_lengths)),\n",
    "            'std': float(np.std(total_lengths)),\n",
    "            'p90': float(np.percentile(total_lengths, 90)),\n",
    "            'p95': float(np.percentile(total_lengths, 95)),\n",
    "            'p99': float(np.percentile(total_lengths, 99))\n",
    "        },\n",
    "        'assistant_stats': {\n",
    "            'min': int(np.min(assistant_lengths)),\n",
    "            'max': int(np.max(assistant_lengths)),\n",
    "            'mean': float(np.mean(assistant_lengths)),\n",
    "            'median': float(np.median(assistant_lengths))\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Print analysis results\n",
    "    print(f\"\\n=== Sequence Length Analysis Results ===\")\n",
    "    print(f\"Total samples analyzed: {results['total_samples']}\")\n",
    "    print(f\"Tokenizer used: {'Actual tokenizer' if tokenizer else 'Character-based estimation'}\")\n",
    "\n",
    "    print(f\"\\nðŸ“Š INPUT LENGTH STATS (System + User + Special Tokens):\")\n",
    "    print(f\"  Min: {results['input_stats']['min']} tokens\")\n",
    "    print(f\"  Max: {results['input_stats']['max']} tokens\")\n",
    "    print(f\"  Mean: {results['input_stats']['mean']:.1f} tokens\")\n",
    "    print(f\"  Median: {results['input_stats']['median']:.1f} tokens\")\n",
    "    print(f\"  90th percentile: {results['input_stats']['p90']:.0f} tokens\")\n",
    "    print(f\"  95th percentile: {results['input_stats']['p95']:.0f} tokens\")\n",
    "    print(f\"  99th percentile: {results['input_stats']['p99']:.0f} tokens\")\n",
    "\n",
    "    print(f\"\\nðŸ“Š TOTAL LENGTH STATS (Input + Assistant):\")\n",
    "    print(f\"  Min: {results['total_stats']['min']} tokens\")\n",
    "    print(f\"  Max: {results['total_stats']['max']} tokens\")\n",
    "    print(f\"  Mean: {results['total_stats']['mean']:.1f} tokens\")\n",
    "    print(f\"  Median: {results['total_stats']['median']:.1f} tokens\")\n",
    "    print(f\"  90th percentile: {results['total_stats']['p90']:.0f} tokens\")\n",
    "    print(f\"  95th percentile: {results['total_stats']['p95']:.0f} tokens\")\n",
    "    print(f\"  99th percentile: {results['total_stats']['p99']:.0f} tokens\")\n",
    "\n",
    "    # Calculate coverage percentages for different max_seq_length values\n",
    "    common_lengths = [512, 1024, 1536, 2048, 3072, 4096]\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ COVERAGE ANALYSIS (% samples that fit within max_seq_length):\")\n",
    "    print(\"Based on INPUT length (System + User):\")\n",
    "    for length in common_lengths:\n",
    "        coverage = (input_lengths <= length).mean() * 100\n",
    "        print(f\"  max_seq_length={length}: {coverage:.1f}% of samples\")\n",
    "\n",
    "    print(f\"\\nBased on TOTAL length (Input + Assistant):\")\n",
    "    for length in common_lengths:\n",
    "        coverage = (total_lengths <= length).mean() * 100\n",
    "        print(f\"  max_seq_length={length}: {coverage:.1f}% of samples\")\n",
    "\n",
    "    # Recommendations\n",
    "    print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
    "\n",
    "    # Find optimal length for 95% coverage of input\n",
    "    optimal_input_95 = int(np.ceil(results['input_stats']['p95'] / 128) * 128)  # Round up to nearest 128\n",
    "    optimal_total_95 = int(np.ceil(results['total_stats']['p95'] / 128) * 128)\n",
    "\n",
    "    print(f\"  For 95% input coverage: max_seq_length = {optimal_input_95}\")\n",
    "    print(f\"  For 95% total coverage: max_seq_length = {optimal_total_95}\")\n",
    "    print(f\"  Current setting (1536): covers {(input_lengths <= 1536).mean()*100:.1f}% of inputs\")\n",
    "\n",
    "    if results['input_stats']['p95'] > 1536:\n",
    "        print(f\"  âš ï¸  Your current max_seq_length=1536 may truncate {(input_lengths > 1536).mean()*100:.1f}% of inputs\")\n",
    "        print(f\"  Consider increasing to {optimal_input_95} for better coverage\")\n",
    "    else:\n",
    "        print(f\"  âœ… Your current max_seq_length=1536 looks good for input coverage\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def plot_length_distribution(dataset_file=\"sft_dataset.json\", model_name=\"meta-llama/Llama-3.2-3B\"):\n",
    "    \"\"\"\n",
    "    Create histograms showing length distribution\n",
    "    \"\"\"\n",
    "    print(\"Creating length distribution plots...\")\n",
    "\n",
    "    # This is a simplified version - you'd need to run the analysis first\n",
    "    # and then create plots based on the data\n",
    "    print(\"To create plots, run the analyze_sequence_lengths function first\")\n",
    "    print(\"Then use the returned data to create matplotlib histograms\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Analyze sequence lengths\n",
    "    results = analyze_sequence_lengths()\n",
    "\n",
    "    if results:\n",
    "        print(f\"\\nðŸŽ¯ Quick recommendation for your current setup:\")\n",
    "        print(f\"Your max_seq_length=1536 setting covers approximately:\")\n",
    "        print(f\"- {(np.array([1536]) >= results['input_stats']['p95']).any() and 'More than 95%' or 'Less than 95%'} of input sequences\")\n",
    "        print(f\"- Consider the analysis above to decide if you need to adjust it\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
